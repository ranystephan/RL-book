{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford CME 241 (Winter 2025) - Assignment 1\n",
    "\n",
    "**Due: Sunday, January 19 @ 11:59 PM PST on Gradescope.**\n",
    "\n",
    "Assignment instructions:\n",
    "- Make sure each of the subquestions have answers\n",
    "- Ensure that group members indicate which problems they're in charge of\n",
    "- Show work and walk through your thought process where applicable\n",
    "- Empty code blocks are for your use, so feel free to create more under each section as needed\n",
    "- Document code with light comments (i.e. 'this function handles visualization')\n",
    "\n",
    "Submission instructions:\n",
    "- When complete, fill out your publicly available GitHub repo file URL and group members below, then export or print this .ipynb file to PDF and upload the PDF to Gradescope.\n",
    "\n",
    "*Link to this ipynb file in your public GitHub repo (replace below URL with yours):* \n",
    "\n",
    "https://github.com/ranystephan/RL-book/blob/master/ranycs_assts_solving/ranycs_asst1/ranycs_assignment1.ipynb\n",
    "\n",
    "https://github.com/cocosrv/technical-documents/blob/master/finance/cme241/assignments/Winter2025/assignment1.ipynb\n",
    "\n",
    "*Group members (replace below names with people in your group):* \n",
    "- Rany Stephan - ranycs@stanford.edu\n",
    "- Corentin Servouze - cosrv@stanford.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in /Users/ranystephan/Desktop/problemst/cme241/RL-book/venv/lib/python3.13/site-packages (0.20.3)\n",
      "Requirement already satisfied: numpy in /Users/ranystephan/Desktop/problemst/cme241/RL-book/venv/lib/python3.13/site-packages (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install graphviz\n",
    "!pip3 install numpy\n",
    "import sys \n",
    "sys.path.append('/Users/ranystephan/Desktop/problemst/cme241/RL-book/')\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Snakes and Ladders (Led by Rany)\n",
    "\n",
    "In the classic childhood game of Snakes and Ladders, all players start to the left of square 1 (call this position 0) and roll a 6-sided die to represent the number of squares they can move forward. The goal is to reach square 100 as quickly as possible. Landing on the bottom rung of a ladder allows for an automatic free-pass to climb, e.g. square 4 sends you directly to 14; whereas landing on a snake's head forces one to slide all the way to the tail, e.g. square 34 sends you to 6. Note, this game can be viewed as a Markov Process, where the outcome is only depedent on the current state and not the prior trajectory. In this question, we will ask you to both formally describe the Markov Process that describes this game, followed by coding up a version of the game to get familiar with the RL-book libraries.\n",
    "\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "How can we model this problem with a Markov Process?\n",
    "\n",
    "---\n",
    "\n",
    "### Subquestions\n",
    "\n",
    "#### Part (A): MDP Modeling\n",
    "\n",
    "Formalize the state space of the Snakes and Ladders game. Don't forget to specify the terminal state!\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (B): Transition Probabilities\n",
    "\n",
    "Write out the structure of the transition probabilities. Feel free to abbreviate all squares that do not have a snake or ladder.\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (C): Modeling the Game\n",
    "\n",
    "Code up a `transition_map: Transition[S]` data structure to represent the transition probabilities of the Snakes and Ladders Markov Process so you can model the game as an instance of `FiniteMarkovProcess`. Use the `traces` method to create sampling traces, and plot the graph of the distribution of time steps to finish the game. Use the image below for the locations of the snakes and ladders.\n",
    "\n",
    "![Snakes and Ladders](./Figures/snakesAndLadders.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "<span style=\"color:red\">\n",
    "\n",
    "\n",
    "The state space $S$ consists of all the squares on the board, including the starting position (0) and the terminal state (100). Therefore, $$ S = \\{0, 1, 2, \\ldots, 100\\} $$\n",
    "\n",
    "The *terminal state* is the square 100, which is the goal of the game. Once a player reaches this state, the game ends.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer\n",
    "\n",
    "<span style=\"color:red\">\n",
    "The transition probabilities for each square can be defined as follows:\n",
    "\n",
    "- For squares without snakes or ladders, the transition probability is uniformly distributed over the next 6 squares (or fewer if near the end of the board).\n",
    "- For squares with ladders, the transition probability is 1 for the square at the top of the ladder.\n",
    "- For squares with snakes, the transition probability is 1 for the square at the tail of the snake.\n",
    "\n",
    "For example, if a player is on square 1, the transition probabilities are:\n",
    "- P(2|1) = 1/6\n",
    "- P(3|1) = 1/6\n",
    "- P(4|1) = 1/6\n",
    "- P(5|1) = 1/6\n",
    "- P(6|1) = 1/6\n",
    "- P(7|1) = 1/6\n",
    "\n",
    "We could also write it as this. If a player is on square k, the transition probabilities are:\n",
    "- P(k+1|k) = 1/6\n",
    "- P(k+2|k) = 1/6\n",
    "- P(k+3|k) = 1/6\n",
    "- P(k+4|k) = 1/6\n",
    "- P(k+5|k) = 1/6\n",
    "- P(k+6|k) = 1/6\n",
    "\n",
    "If a player is on square 4 (bottom of a ladder to 14), the transition probability is:\n",
    "- P(14|4) = 1\n",
    "\n",
    "If a player is on square 34 (head of a snake to 6), the transition probability is:\n",
    "- P(6|34) = 1\n",
    "\n",
    "\n",
    "We can therefore have $P(s'|s) = \\frac{1}{6}$ if $s' = s+r$.\n",
    "\n",
    "Now, if $s\\geq 94$, we have to ensure that players to not exceed position 100. For example, $T(97, 100) = \\frac{2}{6}$ because rolling 3 or more lands the player at 100. \n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('/Users/ranystephan/Desktop/problemst/cme241/RL-book/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From State 0:\n",
      "  To State 1 with Probability 0.167\n",
      "  To State 2 with Probability 0.167\n",
      "  To State 3 with Probability 0.167\n",
      "  To State 14 with Probability 0.167\n",
      "  To State 5 with Probability 0.167\n",
      "  To State 6 with Probability 0.167\n",
      "From State 1:\n",
      "  To State 2 with Probability 0.167\n",
      "  To State 3 with Probability 0.167\n",
      "  To State 14 with Probability 0.167\n",
      "  To State 5 with Probability 0.167\n",
      "  To State 6 with Probability 0.167\n",
      "  To State 7 with Probability 0.167\n",
      "From State 2:\n",
      "  To State 3 with Probability 0.167\n",
      "  To State 14 with Probability 0.167\n",
      "  To State 5 with Probability 0.167\n",
      "  To State 6 with Probability 0.167\n",
      "  To State 7 with Probability 0.167\n",
      "  To State 20 with Probability 0.167\n",
      "From State 3:\n",
      "  To State 14 with Probability 0.167\n",
      "  To State 5 with Probability 0.167\n",
      "  To State 6 with Probability 0.167\n",
      "  To State 7 with Probability 0.167\n",
      "  To State 20 with Probability 0.167\n",
      "  To State 9 with Probability 0.167\n",
      "From State 4:\n",
      "  To State 5 with Probability 0.167\n",
      "  To State 6 with Probability 0.167\n",
      "  To State 7 with Probability 0.167\n",
      "  To State 20 with Probability 0.167\n",
      "  To State 9 with Probability 0.167\n",
      "  To State 10 with Probability 0.167\n",
      "From State 5:\n",
      "  To State 6 with Probability 0.167\n",
      "  To State 7 with Probability 0.167\n",
      "  To State 20 with Probability 0.167\n",
      "  To State 9 with Probability 0.167\n",
      "  To State 10 with Probability 0.167\n",
      "  To State 11 with Probability 0.167\n",
      "From State 6:\n",
      "  To State 7 with Probability 0.167\n",
      "  To State 20 with Probability 0.167\n",
      "  To State 9 with Probability 0.167\n",
      "  To State 10 with Probability 0.167\n",
      "  To State 11 with Probability 0.167\n",
      "  To State 12 with Probability 0.167\n",
      "From State 7:\n",
      "  To State 20 with Probability 0.167\n",
      "  To State 9 with Probability 0.167\n",
      "  To State 10 with Probability 0.167\n",
      "  To State 11 with Probability 0.167\n",
      "  To State 12 with Probability 0.167\n",
      "  To State 13 with Probability 0.167\n",
      "From State 8:\n",
      "  To State 9 with Probability 0.167\n",
      "  To State 10 with Probability 0.167\n",
      "  To State 11 with Probability 0.167\n",
      "  To State 12 with Probability 0.167\n",
      "  To State 13 with Probability 0.167\n",
      "  To State 14 with Probability 0.167\n",
      "From State 9:\n",
      "  To State 10 with Probability 0.167\n",
      "  To State 11 with Probability 0.167\n",
      "  To State 12 with Probability 0.167\n",
      "  To State 13 with Probability 0.167\n",
      "  To State 14 with Probability 0.167\n",
      "  To State 15 with Probability 0.167\n",
      "From State 10:\n",
      "  To State 11 with Probability 0.167\n",
      "  To State 12 with Probability 0.167\n",
      "  To State 13 with Probability 0.167\n",
      "  To State 14 with Probability 0.167\n",
      "  To State 15 with Probability 0.167\n",
      "  To State 16 with Probability 0.167\n",
      "From State 11:\n",
      "  To State 12 with Probability 0.167\n",
      "  To State 13 with Probability 0.167\n",
      "  To State 14 with Probability 0.167\n",
      "  To State 15 with Probability 0.167\n",
      "  To State 16 with Probability 0.167\n",
      "  To State 17 with Probability 0.167\n",
      "From State 12:\n",
      "  To State 13 with Probability 0.167\n",
      "  To State 14 with Probability 0.167\n",
      "  To State 15 with Probability 0.167\n",
      "  To State 16 with Probability 0.167\n",
      "  To State 17 with Probability 0.167\n",
      "  To State 18 with Probability 0.167\n",
      "From State 13:\n",
      "  To State 14 with Probability 0.167\n",
      "  To State 15 with Probability 0.167\n",
      "  To State 16 with Probability 0.167\n",
      "  To State 17 with Probability 0.167\n",
      "  To State 18 with Probability 0.167\n",
      "  To State 19 with Probability 0.167\n",
      "From State 14:\n",
      "  To State 15 with Probability 0.167\n",
      "  To State 16 with Probability 0.167\n",
      "  To State 17 with Probability 0.167\n",
      "  To State 18 with Probability 0.167\n",
      "  To State 19 with Probability 0.167\n",
      "  To State 20 with Probability 0.167\n",
      "From State 15:\n",
      "  To State 16 with Probability 0.167\n",
      "  To State 17 with Probability 0.167\n",
      "  To State 18 with Probability 0.167\n",
      "  To State 19 with Probability 0.167\n",
      "  To State 20 with Probability 0.167\n",
      "  To State 42 with Probability 0.167\n",
      "From State 16:\n",
      "  To State 17 with Probability 0.167\n",
      "  To State 18 with Probability 0.167\n",
      "  To State 19 with Probability 0.167\n",
      "  To State 20 with Probability 0.167\n",
      "  To State 42 with Probability 0.167\n",
      "  To State 22 with Probability 0.167\n",
      "From State 17:\n",
      "  To State 18 with Probability 0.167\n",
      "  To State 19 with Probability 0.167\n",
      "  To State 20 with Probability 0.167\n",
      "  To State 42 with Probability 0.167\n",
      "  To State 22 with Probability 0.167\n",
      "  To State 23 with Probability 0.167\n",
      "From State 18:\n",
      "  To State 19 with Probability 0.167\n",
      "  To State 20 with Probability 0.167\n",
      "  To State 42 with Probability 0.167\n",
      "  To State 22 with Probability 0.167\n",
      "  To State 23 with Probability 0.167\n",
      "  To State 24 with Probability 0.167\n",
      "From State 19:\n",
      "  To State 20 with Probability 0.167\n",
      "  To State 42 with Probability 0.167\n",
      "  To State 22 with Probability 0.167\n",
      "  To State 23 with Probability 0.167\n",
      "  To State 24 with Probability 0.167\n",
      "  To State 25 with Probability 0.167\n",
      "From State 20:\n",
      "  To State 42 with Probability 0.167\n",
      "  To State 22 with Probability 0.167\n",
      "  To State 23 with Probability 0.167\n",
      "  To State 24 with Probability 0.167\n",
      "  To State 25 with Probability 0.167\n",
      "  To State 26 with Probability 0.167\n",
      "From State 21:\n",
      "  To State 22 with Probability 0.167\n",
      "  To State 23 with Probability 0.167\n",
      "  To State 24 with Probability 0.167\n",
      "  To State 25 with Probability 0.167\n",
      "  To State 26 with Probability 0.167\n",
      "  To State 27 with Probability 0.167\n",
      "From State 22:\n",
      "  To State 23 with Probability 0.167\n",
      "  To State 24 with Probability 0.167\n",
      "  To State 25 with Probability 0.167\n",
      "  To State 26 with Probability 0.167\n",
      "  To State 27 with Probability 0.167\n",
      "  To State 76 with Probability 0.167\n",
      "From State 23:\n",
      "  To State 24 with Probability 0.167\n",
      "  To State 25 with Probability 0.167\n",
      "  To State 26 with Probability 0.167\n",
      "  To State 27 with Probability 0.167\n",
      "  To State 76 with Probability 0.167\n",
      "  To State 29 with Probability 0.167\n",
      "From State 24:\n",
      "  To State 25 with Probability 0.167\n",
      "  To State 26 with Probability 0.167\n",
      "  To State 27 with Probability 0.167\n",
      "  To State 76 with Probability 0.167\n",
      "  To State 29 with Probability 0.167\n",
      "  To State 30 with Probability 0.167\n",
      "From State 25:\n",
      "  To State 26 with Probability 0.167\n",
      "  To State 27 with Probability 0.167\n",
      "  To State 76 with Probability 0.167\n",
      "  To State 29 with Probability 0.167\n",
      "  To State 30 with Probability 0.167\n",
      "  To State 31 with Probability 0.167\n",
      "From State 26:\n",
      "  To State 27 with Probability 0.167\n",
      "  To State 76 with Probability 0.167\n",
      "  To State 29 with Probability 0.167\n",
      "  To State 30 with Probability 0.167\n",
      "  To State 31 with Probability 0.167\n",
      "  To State 10 with Probability 0.167\n",
      "From State 27:\n",
      "  To State 76 with Probability 0.167\n",
      "  To State 29 with Probability 0.167\n",
      "  To State 30 with Probability 0.167\n",
      "  To State 31 with Probability 0.167\n",
      "  To State 10 with Probability 0.167\n",
      "  To State 33 with Probability 0.167\n",
      "From State 28:\n",
      "  To State 29 with Probability 0.167\n",
      "  To State 30 with Probability 0.167\n",
      "  To State 31 with Probability 0.167\n",
      "  To State 10 with Probability 0.167\n",
      "  To State 33 with Probability 0.167\n",
      "  To State 34 with Probability 0.167\n",
      "From State 29:\n",
      "  To State 30 with Probability 0.167\n",
      "  To State 31 with Probability 0.167\n",
      "  To State 10 with Probability 0.167\n",
      "  To State 33 with Probability 0.167\n",
      "  To State 34 with Probability 0.167\n",
      "  To State 35 with Probability 0.167\n",
      "From State 30:\n",
      "  To State 31 with Probability 0.167\n",
      "  To State 10 with Probability 0.167\n",
      "  To State 33 with Probability 0.167\n",
      "  To State 34 with Probability 0.167\n",
      "  To State 35 with Probability 0.167\n",
      "  To State 6 with Probability 0.167\n",
      "From State 31:\n",
      "  To State 10 with Probability 0.167\n",
      "  To State 33 with Probability 0.167\n",
      "  To State 34 with Probability 0.167\n",
      "  To State 35 with Probability 0.167\n",
      "  To State 6 with Probability 0.167\n",
      "  To State 37 with Probability 0.167\n",
      "From State 32:\n",
      "  To State 33 with Probability 0.167\n",
      "  To State 34 with Probability 0.167\n",
      "  To State 35 with Probability 0.167\n",
      "  To State 6 with Probability 0.167\n",
      "  To State 37 with Probability 0.167\n",
      "  To State 38 with Probability 0.167\n",
      "From State 33:\n",
      "  To State 34 with Probability 0.167\n",
      "  To State 35 with Probability 0.167\n",
      "  To State 6 with Probability 0.167\n",
      "  To State 37 with Probability 0.167\n",
      "  To State 38 with Probability 0.167\n",
      "  To State 39 with Probability 0.167\n",
      "From State 34:\n",
      "  To State 35 with Probability 0.167\n",
      "  To State 6 with Probability 0.167\n",
      "  To State 37 with Probability 0.167\n",
      "  To State 38 with Probability 0.167\n",
      "  To State 39 with Probability 0.167\n",
      "  To State 40 with Probability 0.167\n",
      "From State 35:\n",
      "  To State 6 with Probability 0.167\n",
      "  To State 37 with Probability 0.167\n",
      "  To State 38 with Probability 0.167\n",
      "  To State 39 with Probability 0.167\n",
      "  To State 40 with Probability 0.167\n",
      "  To State 41 with Probability 0.167\n",
      "From State 36:\n",
      "  To State 37 with Probability 0.167\n",
      "  To State 38 with Probability 0.167\n",
      "  To State 39 with Probability 0.167\n",
      "  To State 40 with Probability 0.167\n",
      "  To State 41 with Probability 0.167\n",
      "  To State 42 with Probability 0.167\n",
      "From State 37:\n",
      "  To State 38 with Probability 0.167\n",
      "  To State 39 with Probability 0.167\n",
      "  To State 40 with Probability 0.167\n",
      "  To State 41 with Probability 0.167\n",
      "  To State 42 with Probability 0.167\n",
      "  To State 43 with Probability 0.167\n",
      "From State 38:\n",
      "  To State 39 with Probability 0.167\n",
      "  To State 40 with Probability 0.167\n",
      "  To State 41 with Probability 0.167\n",
      "  To State 42 with Probability 0.167\n",
      "  To State 43 with Probability 0.167\n",
      "  To State 44 with Probability 0.167\n",
      "From State 39:\n",
      "  To State 40 with Probability 0.167\n",
      "  To State 41 with Probability 0.167\n",
      "  To State 42 with Probability 0.167\n",
      "  To State 43 with Probability 0.167\n",
      "  To State 44 with Probability 0.167\n",
      "  To State 45 with Probability 0.167\n",
      "From State 40:\n",
      "  To State 41 with Probability 0.167\n",
      "  To State 42 with Probability 0.167\n",
      "  To State 43 with Probability 0.167\n",
      "  To State 44 with Probability 0.167\n",
      "  To State 45 with Probability 0.167\n",
      "  To State 46 with Probability 0.167\n",
      "From State 41:\n",
      "  To State 42 with Probability 0.167\n",
      "  To State 43 with Probability 0.167\n",
      "  To State 44 with Probability 0.167\n",
      "  To State 45 with Probability 0.167\n",
      "  To State 46 with Probability 0.167\n",
      "  To State 47 with Probability 0.167\n",
      "From State 42:\n",
      "  To State 43 with Probability 0.167\n",
      "  To State 44 with Probability 0.167\n",
      "  To State 45 with Probability 0.167\n",
      "  To State 46 with Probability 0.167\n",
      "  To State 47 with Probability 0.167\n",
      "  To State 26 with Probability 0.167\n",
      "From State 43:\n",
      "  To State 44 with Probability 0.167\n",
      "  To State 45 with Probability 0.167\n",
      "  To State 46 with Probability 0.167\n",
      "  To State 47 with Probability 0.167\n",
      "  To State 26 with Probability 0.167\n",
      "  To State 49 with Probability 0.167\n",
      "From State 44:\n",
      "  To State 45 with Probability 0.167\n",
      "  To State 46 with Probability 0.167\n",
      "  To State 47 with Probability 0.167\n",
      "  To State 26 with Probability 0.167\n",
      "  To State 49 with Probability 0.167\n",
      "  To State 67 with Probability 0.167\n",
      "From State 45:\n",
      "  To State 46 with Probability 0.167\n",
      "  To State 47 with Probability 0.167\n",
      "  To State 26 with Probability 0.167\n",
      "  To State 49 with Probability 0.167\n",
      "  To State 67 with Probability 0.167\n",
      "  To State 51 with Probability 0.167\n",
      "From State 46:\n",
      "  To State 47 with Probability 0.167\n",
      "  To State 26 with Probability 0.167\n",
      "  To State 49 with Probability 0.167\n",
      "  To State 67 with Probability 0.167\n",
      "  To State 51 with Probability 0.167\n",
      "  To State 52 with Probability 0.167\n",
      "From State 47:\n",
      "  To State 26 with Probability 0.167\n",
      "  To State 49 with Probability 0.167\n",
      "  To State 67 with Probability 0.167\n",
      "  To State 51 with Probability 0.167\n",
      "  To State 52 with Probability 0.167\n",
      "  To State 53 with Probability 0.167\n",
      "From State 48:\n",
      "  To State 49 with Probability 0.167\n",
      "  To State 67 with Probability 0.167\n",
      "  To State 51 with Probability 0.167\n",
      "  To State 52 with Probability 0.167\n",
      "  To State 53 with Probability 0.167\n",
      "  To State 54 with Probability 0.167\n",
      "From State 49:\n",
      "  To State 67 with Probability 0.167\n",
      "  To State 51 with Probability 0.167\n",
      "  To State 52 with Probability 0.167\n",
      "  To State 53 with Probability 0.167\n",
      "  To State 54 with Probability 0.167\n",
      "  To State 55 with Probability 0.167\n",
      "From State 50:\n",
      "  To State 51 with Probability 0.167\n",
      "  To State 52 with Probability 0.167\n",
      "  To State 53 with Probability 0.167\n",
      "  To State 54 with Probability 0.167\n",
      "  To State 55 with Probability 0.167\n",
      "  To State 56 with Probability 0.167\n",
      "From State 51:\n",
      "  To State 52 with Probability 0.167\n",
      "  To State 53 with Probability 0.167\n",
      "  To State 54 with Probability 0.167\n",
      "  To State 55 with Probability 0.167\n",
      "  To State 56 with Probability 0.167\n",
      "  To State 57 with Probability 0.167\n",
      "From State 52:\n",
      "  To State 53 with Probability 0.167\n",
      "  To State 54 with Probability 0.167\n",
      "  To State 55 with Probability 0.167\n",
      "  To State 56 with Probability 0.167\n",
      "  To State 57 with Probability 0.167\n",
      "  To State 58 with Probability 0.167\n",
      "From State 53:\n",
      "  To State 54 with Probability 0.167\n",
      "  To State 55 with Probability 0.167\n",
      "  To State 56 with Probability 0.167\n",
      "  To State 57 with Probability 0.167\n",
      "  To State 58 with Probability 0.167\n",
      "  To State 59 with Probability 0.167\n",
      "From State 54:\n",
      "  To State 55 with Probability 0.167\n",
      "  To State 56 with Probability 0.167\n",
      "  To State 57 with Probability 0.167\n",
      "  To State 58 with Probability 0.167\n",
      "  To State 59 with Probability 0.167\n",
      "  To State 60 with Probability 0.167\n",
      "From State 55:\n",
      "  To State 56 with Probability 0.167\n",
      "  To State 57 with Probability 0.167\n",
      "  To State 58 with Probability 0.167\n",
      "  To State 59 with Probability 0.167\n",
      "  To State 60 with Probability 0.167\n",
      "  To State 61 with Probability 0.167\n",
      "From State 56:\n",
      "  To State 57 with Probability 0.167\n",
      "  To State 58 with Probability 0.167\n",
      "  To State 59 with Probability 0.167\n",
      "  To State 60 with Probability 0.167\n",
      "  To State 61 with Probability 0.167\n",
      "  To State 18 with Probability 0.167\n",
      "From State 57:\n",
      "  To State 58 with Probability 0.167\n",
      "  To State 59 with Probability 0.167\n",
      "  To State 60 with Probability 0.167\n",
      "  To State 61 with Probability 0.167\n",
      "  To State 18 with Probability 0.167\n",
      "  To State 63 with Probability 0.167\n",
      "From State 58:\n",
      "  To State 59 with Probability 0.167\n",
      "  To State 60 with Probability 0.167\n",
      "  To State 61 with Probability 0.167\n",
      "  To State 18 with Probability 0.167\n",
      "  To State 63 with Probability 0.167\n",
      "  To State 64 with Probability 0.167\n",
      "From State 59:\n",
      "  To State 60 with Probability 0.167\n",
      "  To State 61 with Probability 0.167\n",
      "  To State 18 with Probability 0.167\n",
      "  To State 63 with Probability 0.167\n",
      "  To State 64 with Probability 0.167\n",
      "  To State 65 with Probability 0.167\n",
      "From State 60:\n",
      "  To State 61 with Probability 0.167\n",
      "  To State 18 with Probability 0.167\n",
      "  To State 63 with Probability 0.167\n",
      "  To State 64 with Probability 0.167\n",
      "  To State 65 with Probability 0.167\n",
      "  To State 66 with Probability 0.167\n",
      "From State 61:\n",
      "  To State 18 with Probability 0.167\n",
      "  To State 63 with Probability 0.167\n",
      "  To State 64 with Probability 0.167\n",
      "  To State 65 with Probability 0.167\n",
      "  To State 66 with Probability 0.167\n",
      "  To State 67 with Probability 0.167\n",
      "From State 62:\n",
      "  To State 63 with Probability 0.167\n",
      "  To State 64 with Probability 0.167\n",
      "  To State 65 with Probability 0.167\n",
      "  To State 66 with Probability 0.167\n",
      "  To State 67 with Probability 0.167\n",
      "  To State 68 with Probability 0.167\n",
      "From State 63:\n",
      "  To State 64 with Probability 0.167\n",
      "  To State 65 with Probability 0.167\n",
      "  To State 66 with Probability 0.167\n",
      "  To State 67 with Probability 0.167\n",
      "  To State 68 with Probability 0.167\n",
      "  To State 69 with Probability 0.167\n",
      "From State 64:\n",
      "  To State 65 with Probability 0.167\n",
      "  To State 66 with Probability 0.167\n",
      "  To State 67 with Probability 0.167\n",
      "  To State 68 with Probability 0.167\n",
      "  To State 69 with Probability 0.167\n",
      "  To State 70 with Probability 0.167\n",
      "From State 65:\n",
      "  To State 66 with Probability 0.167\n",
      "  To State 67 with Probability 0.167\n",
      "  To State 68 with Probability 0.167\n",
      "  To State 69 with Probability 0.167\n",
      "  To State 70 with Probability 0.167\n",
      "  To State 92 with Probability 0.167\n",
      "From State 66:\n",
      "  To State 67 with Probability 0.167\n",
      "  To State 68 with Probability 0.167\n",
      "  To State 69 with Probability 0.167\n",
      "  To State 70 with Probability 0.167\n",
      "  To State 92 with Probability 0.167\n",
      "  To State 72 with Probability 0.167\n",
      "From State 67:\n",
      "  To State 68 with Probability 0.167\n",
      "  To State 69 with Probability 0.167\n",
      "  To State 70 with Probability 0.167\n",
      "  To State 92 with Probability 0.167\n",
      "  To State 72 with Probability 0.167\n",
      "  To State 73 with Probability 0.167\n",
      "From State 68:\n",
      "  To State 69 with Probability 0.167\n",
      "  To State 70 with Probability 0.167\n",
      "  To State 92 with Probability 0.167\n",
      "  To State 72 with Probability 0.167\n",
      "  To State 73 with Probability 0.167\n",
      "  To State 74 with Probability 0.167\n",
      "From State 69:\n",
      "  To State 70 with Probability 0.167\n",
      "  To State 92 with Probability 0.167\n",
      "  To State 72 with Probability 0.167\n",
      "  To State 73 with Probability 0.167\n",
      "  To State 74 with Probability 0.167\n",
      "  To State 75 with Probability 0.167\n",
      "From State 70:\n",
      "  To State 92 with Probability 0.167\n",
      "  To State 72 with Probability 0.167\n",
      "  To State 73 with Probability 0.167\n",
      "  To State 74 with Probability 0.167\n",
      "  To State 75 with Probability 0.167\n",
      "  To State 76 with Probability 0.167\n",
      "From State 71:\n",
      "  To State 72 with Probability 0.167\n",
      "  To State 73 with Probability 0.167\n",
      "  To State 74 with Probability 0.167\n",
      "  To State 75 with Probability 0.167\n",
      "  To State 76 with Probability 0.167\n",
      "  To State 77 with Probability 0.167\n",
      "From State 72:\n",
      "  To State 73 with Probability 0.167\n",
      "  To State 74 with Probability 0.167\n",
      "  To State 75 with Probability 0.167\n",
      "  To State 76 with Probability 0.167\n",
      "  To State 77 with Probability 0.167\n",
      "  To State 78 with Probability 0.167\n",
      "From State 73:\n",
      "  To State 74 with Probability 0.167\n",
      "  To State 75 with Probability 0.167\n",
      "  To State 76 with Probability 0.167\n",
      "  To State 77 with Probability 0.167\n",
      "  To State 78 with Probability 0.167\n",
      "  To State 79 with Probability 0.167\n",
      "From State 74:\n",
      "  To State 75 with Probability 0.167\n",
      "  To State 76 with Probability 0.167\n",
      "  To State 77 with Probability 0.167\n",
      "  To State 78 with Probability 0.167\n",
      "  To State 79 with Probability 0.167\n",
      "  To State 99 with Probability 0.167\n",
      "From State 75:\n",
      "  To State 76 with Probability 0.167\n",
      "  To State 77 with Probability 0.167\n",
      "  To State 78 with Probability 0.167\n",
      "  To State 79 with Probability 0.167\n",
      "  To State 99 with Probability 0.167\n",
      "  To State 81 with Probability 0.167\n",
      "From State 76:\n",
      "  To State 77 with Probability 0.167\n",
      "  To State 78 with Probability 0.167\n",
      "  To State 79 with Probability 0.167\n",
      "  To State 99 with Probability 0.167\n",
      "  To State 81 with Probability 0.167\n",
      "  To State 82 with Probability 0.167\n",
      "From State 77:\n",
      "  To State 78 with Probability 0.167\n",
      "  To State 79 with Probability 0.167\n",
      "  To State 99 with Probability 0.167\n",
      "  To State 81 with Probability 0.167\n",
      "  To State 82 with Probability 0.167\n",
      "  To State 83 with Probability 0.167\n",
      "From State 78:\n",
      "  To State 79 with Probability 0.167\n",
      "  To State 99 with Probability 0.167\n",
      "  To State 81 with Probability 0.167\n",
      "  To State 82 with Probability 0.167\n",
      "  To State 83 with Probability 0.167\n",
      "  To State 84 with Probability 0.167\n",
      "From State 79:\n",
      "  To State 99 with Probability 0.167\n",
      "  To State 81 with Probability 0.167\n",
      "  To State 82 with Probability 0.167\n",
      "  To State 83 with Probability 0.167\n",
      "  To State 84 with Probability 0.167\n",
      "  To State 85 with Probability 0.167\n",
      "From State 80:\n",
      "  To State 81 with Probability 0.167\n",
      "  To State 82 with Probability 0.167\n",
      "  To State 83 with Probability 0.167\n",
      "  To State 84 with Probability 0.167\n",
      "  To State 85 with Probability 0.167\n",
      "  To State 86 with Probability 0.167\n",
      "From State 81:\n",
      "  To State 82 with Probability 0.167\n",
      "  To State 83 with Probability 0.167\n",
      "  To State 84 with Probability 0.167\n",
      "  To State 85 with Probability 0.167\n",
      "  To State 86 with Probability 0.167\n",
      "  To State 87 with Probability 0.167\n",
      "From State 82:\n",
      "  To State 83 with Probability 0.167\n",
      "  To State 84 with Probability 0.167\n",
      "  To State 85 with Probability 0.167\n",
      "  To State 86 with Probability 0.167\n",
      "  To State 87 with Probability 0.167\n",
      "  To State 24 with Probability 0.167\n",
      "From State 83:\n",
      "  To State 84 with Probability 0.167\n",
      "  To State 85 with Probability 0.167\n",
      "  To State 86 with Probability 0.167\n",
      "  To State 87 with Probability 0.167\n",
      "  To State 24 with Probability 0.167\n",
      "  To State 89 with Probability 0.167\n",
      "From State 84:\n",
      "  To State 85 with Probability 0.167\n",
      "  To State 86 with Probability 0.167\n",
      "  To State 87 with Probability 0.167\n",
      "  To State 24 with Probability 0.167\n",
      "  To State 89 with Probability 0.167\n",
      "  To State 90 with Probability 0.167\n",
      "From State 85:\n",
      "  To State 86 with Probability 0.167\n",
      "  To State 87 with Probability 0.167\n",
      "  To State 24 with Probability 0.167\n",
      "  To State 89 with Probability 0.167\n",
      "  To State 90 with Probability 0.167\n",
      "  To State 91 with Probability 0.167\n",
      "From State 86:\n",
      "  To State 87 with Probability 0.167\n",
      "  To State 24 with Probability 0.167\n",
      "  To State 89 with Probability 0.167\n",
      "  To State 90 with Probability 0.167\n",
      "  To State 91 with Probability 0.167\n",
      "  To State 92 with Probability 0.167\n",
      "From State 87:\n",
      "  To State 24 with Probability 0.167\n",
      "  To State 89 with Probability 0.167\n",
      "  To State 90 with Probability 0.167\n",
      "  To State 91 with Probability 0.167\n",
      "  To State 92 with Probability 0.167\n",
      "  To State 93 with Probability 0.167\n",
      "From State 88:\n",
      "  To State 89 with Probability 0.167\n",
      "  To State 90 with Probability 0.167\n",
      "  To State 91 with Probability 0.167\n",
      "  To State 92 with Probability 0.167\n",
      "  To State 93 with Probability 0.167\n",
      "  To State 94 with Probability 0.167\n",
      "From State 89:\n",
      "  To State 90 with Probability 0.167\n",
      "  To State 91 with Probability 0.167\n",
      "  To State 92 with Probability 0.167\n",
      "  To State 93 with Probability 0.167\n",
      "  To State 94 with Probability 0.167\n",
      "  To State 56 with Probability 0.167\n",
      "From State 90:\n",
      "  To State 91 with Probability 0.167\n",
      "  To State 92 with Probability 0.167\n",
      "  To State 93 with Probability 0.167\n",
      "  To State 94 with Probability 0.167\n",
      "  To State 56 with Probability 0.167\n",
      "  To State 96 with Probability 0.167\n",
      "From State 91:\n",
      "  To State 92 with Probability 0.167\n",
      "  To State 93 with Probability 0.167\n",
      "  To State 94 with Probability 0.167\n",
      "  To State 56 with Probability 0.167\n",
      "  To State 96 with Probability 0.167\n",
      "  To State 78 with Probability 0.167\n",
      "From State 92:\n",
      "  To State 93 with Probability 0.167\n",
      "  To State 94 with Probability 0.167\n",
      "  To State 56 with Probability 0.167\n",
      "  To State 96 with Probability 0.167\n",
      "  To State 78 with Probability 0.167\n",
      "  To State 98 with Probability 0.167\n",
      "From State 93:\n",
      "  To State 94 with Probability 0.167\n",
      "  To State 56 with Probability 0.167\n",
      "  To State 96 with Probability 0.167\n",
      "  To State 78 with Probability 0.167\n",
      "  To State 98 with Probability 0.167\n",
      "  To State 99 with Probability 0.167\n",
      "From State 94:\n",
      "  To State 56 with Probability 0.167\n",
      "  To State 96 with Probability 0.167\n",
      "  To State 78 with Probability 0.167\n",
      "  To State 98 with Probability 0.167\n",
      "  To State 99 with Probability 0.167\n",
      "  To Terminal State 100 with Probability 0.167\n",
      "From State 95:\n",
      "  To State 96 with Probability 0.167\n",
      "  To State 78 with Probability 0.167\n",
      "  To State 98 with Probability 0.167\n",
      "  To State 99 with Probability 0.167\n",
      "  To Terminal State 100 with Probability 0.333\n",
      "From State 96:\n",
      "  To State 78 with Probability 0.167\n",
      "  To State 98 with Probability 0.167\n",
      "  To State 99 with Probability 0.167\n",
      "  To Terminal State 100 with Probability 0.500\n",
      "From State 97:\n",
      "  To State 98 with Probability 0.167\n",
      "  To State 99 with Probability 0.167\n",
      "  To Terminal State 100 with Probability 0.667\n",
      "From State 98:\n",
      "  To State 99 with Probability 0.167\n",
      "  To Terminal State 100 with Probability 0.833\n",
      "From State 99:\n",
      "  To Terminal State 100 with Probability 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#    Key = the square where a snake head or ladder base starts\n",
    "#    Value = the square it takes us to (snake tail or ladder top).\n",
    "snakes_ladders = {\n",
    "    4: 14, 8: 20, 21: 42, 28: 76, 32: 10, 36: 6,\n",
    "    48: 26, 50: 67, 62: 18, 71: 92, 80: 99, 88: 24,\n",
    "    95: 56, 97: 78\n",
    "}\n",
    "\n",
    "# Build a transition_map for states 0..99.\n",
    "#    State 100 will be absorbed as a terminal state.\n",
    "#    This example allows \"overshooting\" 100 => immediate terminal.\n",
    "from collections import defaultdict\n",
    "from typing import Dict\n",
    "from rl.distribution import Categorical\n",
    "from rl.markov_process import NonTerminal, Terminal, FiniteMarkovProcess\n",
    "\n",
    "def build_transition_map() -> Dict[int, Categorical[int]]:\n",
    "    transition_map: Dict[int, Categorical[int]] = {}\n",
    "\n",
    "    for s in range(100):\n",
    "        outcomes = defaultdict(float)\n",
    "        # Each roll has probability 1/6\n",
    "        for roll in range(1, 7):\n",
    "            next_sq = s + roll\n",
    "            # If we land on or past 100, go to 100 (terminal)\n",
    "            if next_sq >= 100:\n",
    "                outcomes[100] += 1/6\n",
    "            else:\n",
    "                # Check for snake or ladder\n",
    "                if next_sq in snakes_ladders:\n",
    "                    next_sq = snakes_ladders[next_sq]\n",
    "                outcomes[next_sq] += 1/6\n",
    "        transition_map[s] = Categorical(outcomes)\n",
    "\n",
    "    return transition_map\n",
    "\n",
    "transition_map = build_transition_map()\n",
    "\n",
    "# 3. Create a FiniteMarkovProcess for Snakes & Ladders\n",
    "snakes_ladders_mp = FiniteMarkovProcess(transition_map)\n",
    "print(snakes_ladders_mp) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from rl.distribution import Constant\n",
    "\n",
    "def simulate_single_game(mdp: FiniteMarkovProcess[int]) -> int:\n",
    "    \"\"\"\n",
    "    Simulates one Snakes & Ladders game starting from state 0\n",
    "    and returns the number of time-steps needed to reach terminal.\n",
    "    \"\"\"\n",
    "    start_dist = Constant(NonTerminal(0))  # always start at NonTerminal(0)\n",
    "    trace_iter = mdp.simulate(start_dist)  # an iterable of states\n",
    "    steps = 0\n",
    "    for state in trace_iter:\n",
    "        # Once we yield a Terminal state, we are done\n",
    "        if isinstance(state, Terminal):\n",
    "            return steps\n",
    "        steps += 1\n",
    "    return steps\n",
    "\n",
    "# 5. We run many simulations, and record the time steps to finish, and plot.\n",
    "NUM_SIMULATIONS = 10_000\n",
    "finishing_times = [\n",
    "    simulate_single_game(snakes_ladders_mp) for _ in range(NUM_SIMULATIONS)\n",
    "]\n",
    "\n",
    "plt.hist(finishing_times, bins=range(min(finishing_times), max(finishing_times) + 2),\n",
    "         alpha=0.7, edgecolor='black', density=True)\n",
    "plt.xlabel(\"Time Steps to Reach 100\")\n",
    "plt.ylabel(\"Frequency (Normalized)\")\n",
    "plt.title(\"Distribution of Time Steps to Finish Snakes & Ladders\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Markov Decision Processes (Led by Rany)\n",
    "\n",
    "Consider an MDP with an infinite set of states $\\mathcal{S} = \\{1,2,3,\\ldots \\}$. The start state is $s=1$. Each state $s$ allows a continuous set of actions $a \\in [0,1]$. The transition probabilities are given by: \n",
    "$$\\mathbb{P}[s+1 \\mid s, a] = a, \\mathbb{P}[s \\mid s, a] = 1 - a \\text{ for all } s \\in \\mathcal{S} \\text{ for all } a \\in [0,1]$$\n",
    "For all states $s \\in \\mathcal{S}$ and actions $a \\in [0,1]$, transitioning from $s$ to $s+1$ results in a reward of $1-a$ and transitioning from $s$ to $s$ results in a reward of $1+a$. The discount factor $\\gamma=0.5$.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "How can we derive a mathematical formulation for the value function and the optimal policy? And how do those functions change when we modify the action space?\n",
    "\n",
    "---\n",
    "\n",
    "### Subquestions\n",
    "\n",
    "#### Part (A): Optimal Value Function  \n",
    "\n",
    "Using the MDP Bellman Optimality Equation, calculate the Optimal Value Function $V^*(s)$ for all $s \\in \\mathcal{S}$. Given $V^*(s)$, what is the optimal action, $a^*$, that maximizes the optimal value function?\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (B): Optimal Policy  \n",
    "\n",
    "Calculate an Optimal Deterministic Policy $\\pi^*(s)$ for all $s \\in \\mathcal{S}$.\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (C): Changing the Action Space  \n",
    "\n",
    "Let's assume that we modify the action space such that instead of $a \\in [0,1]$ for all states, we restrict the action space to $a \\in \\left[0,\\frac{1}{s}\\right]$ for state $s$. This means that higher states have more restricted action spaces. How does this constraint affect:\n",
    "\n",
    "- The form of the Bellman optimality equation?\n",
    "- The optimal value function, $V^*(s)$?\n",
    "- The structure of the optimal policy, $\\pi^*(s)$?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "<span style=\"color:red\">\n",
    "We get here that \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "V^* (s) &= \\max_{a\\in[0,1]} [a(1-a + 0.5 V^* (s+1)) + (1-a)(1+a + 0.5 V^*(s))] \\\\\n",
    "&= \\max_{a\\in[0,1]}[(1+a-2a^2) + 0.5[aV^* (s+1)+(1-a)V^*(s)]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now we can see here that $V^*$ does not depend on $s$. Therefore, $V^*(s)=V^*(s+1)$. \n",
    "\n",
    "We let $V^*(s)=v \\ \\forall s \\in \\mathcal{S}$, then \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v &= \\max_{a\\in[0,1]}[(1+a-2a^2) + 0.5[av+(1-a)v]] \\\\\n",
    "0.5 v &= \\max_{a\\in[0,1]}[1+a-2a^2]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now, to find the max values, we take the first derivative which is equal to $1-4a$, set it to 0. We then get the critical value $a = \\frac{1}{4}$. \n",
    "\n",
    "Finally since the double derivative is equal to $-4$, we know that this critical point yields a maxima. Plugging it in the function, we get that \n",
    "$$\n",
    "V^*(s) = 2.25 \\text{ with } a^* = \\frac{1}{4} \\ \\forall \\ s \\in \\mathcal{S}\n",
    "$$\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer\n",
    "\n",
    "<span style=\"color:red\">\n",
    "\n",
    "Based on our previous question, an optimal deterministic policy $\\pi^* (s)$ is $\\frac{1}{4} \\ \\forall s \\in \\mathcal{S}$ \n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer\n",
    "\n",
    "#### Bellman Optimality Equation Change:\n",
    "<span style=\"color:red\">\n",
    "\n",
    "The Bellman Optimality Equation change would look like this:\n",
    "\n",
    "$$\n",
    "V^* (s) = \\max_{a \\in [0, 1/s]}[a(1-a + 0.5 V^* (s+1)) + (1-a)(1+a + 0.5 V^*(s))]\n",
    "$$\n",
    "\n",
    "In fact, the action space is now state-dependent so the maximizatino is performed over a restricted range of actions. This affects the optimization step but does not change the general form of the equation.\n",
    "\n",
    "</span>\n",
    "\n",
    "#### Optimal Value Function Change:\n",
    "<span style=\"color:red\">\n",
    "\n",
    "\n",
    "This time $V^*(s)$ depends on  $s$ but is decreasing, indeed we can't come back to a previous state and the constraints are only increased. Hence, using the Bellman equation we find that the optimal $a^*$ at state $s$ is given by \n",
    "$$a^* = \\min\\left(\\frac{1}{s},\\frac{1+\\gamma[V^*(s+1)-V^*(s)]}{4}\\right)\\leq \\frac{1}{4}$$\n",
    "It makes sense that the optimal action is decreased because unlikely the first assumption, we would like to prevent the state to increase. Furthermore, at some point we will have $\\forall s\\geq s', \\frac{1}{s}<\\frac{1+\\gamma[V^*(s+1)-V^*(s)]}{4}$, and $\\forall s\\geq s'$\n",
    "\\begin{align*}\n",
    "V^*(s) = \\left(1-\\frac{1}{s}\\right)\\left(1+\\frac{2}{s}\\right)+\\gamma\\left(1-\\frac{1}{s}\\right)V^*(s)+\\gamma\\frac{1}{s}V^*(s+1)\n",
    "\\end{align*}\n",
    "Thanks to this equation, we can deduce the following asymptotic devlopement $V^*(s) = 2+\\frac{2}{s}+o\\left(\\frac{1}{s}\\right)$ \n",
    "\n",
    "\n",
    "We can say that:\n",
    "\n",
    "For small $s$ (ex: s = 1, 2, 3), we still have $\\frac{1}{s} \\geq 0.25$, so we can choose $a = 0.25$ if we want. Indeed we can show that it remains optimal for those smaller $s$. \n",
    "\n",
    "However, once $s$ is large enough that $\\frac{1}{s} < 0.25$, we are forbidden from choosing $a = 0.25$ due to the constraint.\n",
    "\n",
    "Therefore, for $s\\leq 4$, $\\frac{1}{s} \\geq \\frac{1}{4}$, so $a^* = \\frac{1}{4}$ and $V^*(s)=2.25$. \n",
    "\n",
    "Then, for $s>4$, $a^* = \\frac{1}{s}$, and $V^*(s)$ decreases recursively, depending on $V^*(s+1)$. \n",
    "\n",
    "</span>\n",
    "\n",
    "#### Optimal Policy Change:\n",
    "<span style=\"color:red\">\n",
    "\n",
    "For $s \\leq 4$, we can still choose $a = 0.25$, so that it remains optimal. \n",
    "\n",
    "For $s>4$, the largest permitted $a$ is $1/s < 0.25$, so the policy hits the boundary and chooses $a = \\frac{1}{s}$. \n",
    "\n",
    "We can write this as \n",
    "\n",
    "$$\n",
    "\\pi^*(s) = \\min(0.25, \\frac{1}{s})\n",
    "$$\n",
    "\n",
    "Therefore, once $s$ is large enough that $frac{1}{s} < 0.25$, we simply pick the largest feasible action, $\\frac{1}{s}$. \n",
    "\n",
    "\n",
    "Ultimately, restricting $a$ to $[0, \\frac{1}{s}]$ instroduces state dependency in the policy and value function for higher states, reducing achievable reqards and altering the optimal strategy. \n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Frog in a Pond (Led by Corentin)\n",
    "\n",
    "Consider an array of $n+1$ lilypads on a pond, numbered $0$ to $n$. A frog sits on a lilypad other than the lilypads numbered $0$ or $n$. When on lilypad $i$ ($1 \\leq i \\leq n-1$), the frog can croak one of two sounds: **A** or **B**. \n",
    "\n",
    "- If it croaks **A** when on lilypad $i$ ($1 \\leq i \\leq n-1$):\n",
    "  - It is thrown to lilypad $i-1$ with probability $\\frac{i}{n}$.\n",
    "  - It is thrown to lilypad $i+1$ with probability $\\frac{n-i}{n}$.\n",
    "  \n",
    "- If it croaks **B** when on lilypad $i$ ($1 \\leq i \\leq n-1$):\n",
    "  - It is thrown to one of the lilypads $0, \\ldots, i-1, i+1, \\ldots, n$ with uniform probability $\\frac{1}{n}$.\n",
    "\n",
    "A snake, perched on lilypad $0$, will eat the frog if it lands on lilypad $0$. The frog can escape the pond (and hence, escape the snake!) if it lands on lilypad $n$.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "What should the frog croak when on each of the lilypads $1, 2, \\ldots, n-1$, in order to maximize the probability of escaping the pond (i.e., reaching lilypad $n$ before reaching lilypad $0$)? \n",
    "\n",
    "Although there are multiple ways to solve this problem, we aim to solve it by modeling it as a **Markov Decision Process (MDP)** and identifying the **Optimal Policy**.\n",
    "\n",
    "---\n",
    "\n",
    "### Subquestions\n",
    "\n",
    "#### Part (A): MDP Modeling\n",
    "\n",
    "Express the frog-escape problem as an MDP using clear mathematical notation by defining the following components: \n",
    "\n",
    "- **State Space**: Define the possible states of the MDP. \n",
    "- **Action Space**: Specify the actions available to the frog at each state. \n",
    "- **Transition Function**: Describe the probabilities of transitioning between states for each action. \n",
    "- **Reward Function**: Specify the reward associated with the states and transitions. \n",
    "\n",
    "---\n",
    "\n",
    "#### Part (B): Python Implementation\n",
    "\n",
    "There is starter code below to solve this problem programatically. Fill in each of the $6$ `TODO` areas in the code. As a reference for the transition probabilities and rewards, you can make use of the example in slide 16/31 from the following slide deck: https://github.com/coverdrive/technical-documents/blob/master/finance/cme241/Tour-MP.pdf.\n",
    "\n",
    "Write Python code that:\n",
    "\n",
    "- Models this MDP.\n",
    "- Solves the **Optimal Value Function** and the **Optimal Policy**.\n",
    "\n",
    "Feel free to use/adapt code from the textbook. Note, there are other libraries that are needed to actually run this code, so running it will not do anything. Just fill in the code so that it could run assuming that the other libraries are present.\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (C): Visualization and Analysis\n",
    "\n",
    "After running the code, we observe the following graphs for $n=3$, $n=10$, and $n=25$:\n",
    "\n",
    "![FrogGraphs](./Figures/frogGraphs.png)\n",
    "\n",
    "What patterns do you observe for the **Optimal Policy** as you vary $n$ from $3$ to $25$? When the frog is on lilypad $13$ (with $25$ total), what action should the frog take? Is this action different than the action the frog should take if it is on lilypad $1$?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "#### State Space:  \n",
    "<span style=\"color:red\">\n",
    "\n",
    "The state space consists of the positions of the frog on the lilypads. The frog can be on any lilypad from $0$ to $n$, so the state space is:\n",
    "$$\\mathcal{S} = \\{0,1,...,n\\}$$\n",
    "Each state represents the frog's current position on the pond, the state $0$ and $n$ are terminal state.\n",
    "\n",
    "</span>\n",
    "\n",
    "#### Action Space:  \n",
    "\n",
    "<span style=\"color:red\">\n",
    "\n",
    "At each non-terminal state $i$, the frog has two possible actions :\\\n",
    "$\\textbf{Action A}$ : move to $i-1$ with probability $\\frac{i}{n}$ or to $i+1$ with probability $\\frac{n-i}{i}$\\\n",
    "$\\textbf{Action B}$ : move to any lilypad (except $i$) with probability $\\frac{1}{n}$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### Transition Function:  \n",
    "\n",
    "<span style=\"color:red\">\n",
    "\n",
    "We have for $1\\leq i\\leq n-1$\n",
    "\\begin{align*}\n",
    "\\mathcal{P}(i,A,i-1) &= \\frac{i}{n}\\\\\n",
    "\\mathcal{P}(i,A,i+1) &= \\frac{n-i}{n}\\\\\n",
    "\\mathcal{P}(i,B,j) &= \\frac{1}{n}\\mathbb{I}_{j\\neq i}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "</span>\n",
    "\n",
    "#### Reward Function:  \n",
    "\n",
    "<span style=\"color:red\">\n",
    "\n",
    "We know that $n$ and $0$ are terminal states, thus if we set $R_t(s) = 1 \\text{ if }s = n \\text{ else } 0$ and $\\gamma = 1$, we indeed have \n",
    "\\begin{align*}\n",
    "\\mathbb{E}(G_t) &= \\displaystyle\\sum_{t\\geq 0}\\mathbb{E}(R_t)\\\\\n",
    "&=  \\displaystyle\\sum_{t\\geq 0}\\mathbb{P}(S_t = n)\\\\\n",
    "&= \\mathbb{P}(\\text{reach }0\\text{ before }n)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MDPRefined = dict\n",
    "def get_lily_pads_mdp(n: int) -> MDPRefined:\n",
    "    data = {\n",
    "        i: {\n",
    "            'A': {\n",
    "                i - 1: i/n,\n",
    "                i + 1: (n-i)/n,\n",
    "            },\n",
    "            'B': {\n",
    "                j: 1/n for j in range(n + 1) if j != i\n",
    "            }\n",
    "        } for i in range(1, n)\n",
    "    }\n",
    "    data[0] = {'A': {0: 1}, 'B': {0: 1}}\n",
    "    data[n] = {'A': {n: 1}, 'B': {n: 1}}\n",
    "\n",
    "    gamma = 1.0\n",
    "    return MDPRefined(data, gamma)\n",
    "\n",
    "Mapping = dict\n",
    "def direct_bellman(n: int) -> Mapping[int, float]:\n",
    "    vf = [0.5] * (n + 1)\n",
    "    vf[0] = 0.\n",
    "    vf[n] = 0.\n",
    "    tol = 1e-8\n",
    "    epsilon = tol * 1e4\n",
    "    while epsilon >= tol:\n",
    "        old_vf = [v for v in vf]\n",
    "        for i in range(1, n):\n",
    "            value_A = (i / n) * vf[i-1] + ((n - i) / n) * vf[i+1]\n",
    "            value_B = sum(vf[j] for j in range(n + 1) if j != i) / n\n",
    "            vf[i] = max(value_A, value_B)\n",
    "        epsilon = max(abs(old_vf[i] - v) for i, v in enumerate(vf))\n",
    "    return {v: f for v, f in enumerate(vf)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer\n",
    "\n",
    "<span style=\"color:red\">\n",
    "\n",
    "As we vary $n$ from 3 to 25, we observe that action $A$ becomes more favorable as $i$ approaches $n$, while action $B$ is more attractive when $i$ is closer to 0. This trend can be seen in Figure 3, where, when the frog is on lilypad $i = 13$, the optimal action to take is action $A$, which brings the frog closer to escaping the pond. On the other hand, when the frog is on lilypad $i = 1$, it should opt for action $B$, which provides a chance to move away from the dangerous lilypad 0, improving its chances of survival and escape.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Manual Value Iteration (Led by Corentin (mainly) and Rany)\n",
    "\n",
    "Consider a simple MDP with $\\mathcal{S} = \\{s_1, s_2, s_3\\}, \\mathcal{T} = \\{s_3\\}, \\mathcal{A} = \\{a_1, a_2\\}$. The State Transition Probability function  \n",
    "$$\\mathcal{P}: \\mathcal{N} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow [0, 1]$$  \n",
    "is defined as:  \n",
    "$$\\mathcal{P}(s_1, a_1, s_1) = 0.25, \\mathcal{P}(s_1, a_1, s_2) = 0.65, \\mathcal{P}(s_1, a_1, s_3) = 0.1$$  \n",
    "$$\\mathcal{P}(s_1, a_2, s_1) = 0.1, \\mathcal{P}(s_1, a_2, s_2) = 0.4, \\mathcal{P}(s_1, a_2, s_3) = 0.5$$  \n",
    "$$\\mathcal{P}(s_2, a_1, s_1) = 0.3, \\mathcal{P}(s_2, a_1, s_2) = 0.15, \\mathcal{P}(s_2, a_1, s_3) = 0.55$$  \n",
    "$$\\mathcal{P}(s_2, a_2, s_1) = 0.25, \\mathcal{P}(s_2, a_2, s_2) = 0.55, \\mathcal{P}(s_2, a_2, s_3) = 0.2$$  \n",
    "\n",
    "The Reward Function  \n",
    "$$\\mathcal{R}: \\mathcal{N} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$$  \n",
    "is defined as:  \n",
    "$$\\mathcal{R}(s_1, a_1) = 8.0, \\mathcal{R}(s_1, a_2) = 10.0$$  \n",
    "$$\\mathcal{R}(s_2, a_1) = 1.0, \\mathcal{R}(s_2, a_2) = -1.0$$  \n",
    "\n",
    "Assume a discount factor of $\\gamma = 1$.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Your task is to determine an Optimal Deterministic Policy **by manually working out** (not with code) the first two iterations of the Value Iteration algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "### Subquestions\n",
    "\n",
    "#### Part (A): 2 Iterations\n",
    "\n",
    "1. Initialize the Value Function for each state to be its $\\max$ (over actions) reward, i.e., we initialize the Value Function to be $v_0(s_1) = 10.0, v_0(s_2) = 1.0, v_0(s_3) = 0.0$. Then manually calculate $q_k(\\cdot, \\cdot)$ and $v_k(\\cdot)$ from $v_{k - 1}(\\cdot)$ using the Value Iteration update, and then calculate the greedy policy $\\pi_k(\\cdot)$ from $q_k(\\cdot, \\cdot)$ for $k = 1$ and $k = 2$ (hence, 2 iterations).\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (B): Argument\n",
    "\n",
    "1. Now argue that $\\pi_k(\\cdot)$ for $k > 2$ will be the same as $\\pi_2(\\cdot)$. *Hint*: You can make the argument by examining the structure of how you get $q_k(\\cdot, \\cdot)$ from $v_{k-1}(\\cdot)$. With this argument, there is no need to go beyond the two iterations you performed above, and so you can establish $\\pi_2(\\cdot)$ as an Optimal Deterministic Policy for this MDP.\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (C): Policy Evaluation\n",
    "\n",
    "1. Using the policy $\\pi_2(\\cdot)$, compute the exact value function $V^{\\pi_2}(s)$ for all $s\\in S$.\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (D): Sensitivity Analysis\n",
    "\n",
    "Assume the reward for $R(s_1, a_2)$ is modified to $11.0$ instead of $10.0$.\n",
    "\n",
    "1. Perform one iteration of Value Iteration starting from the initialized value function $v_0(s)$, where $v_0(s)$ remains the same as in the original problem.\n",
    "2. Determine whether this change impacts the Optimal Deterministic Policy $\\pi(\\cdot)$. If it does, explain why.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "We initialize the value function as:\n",
    "\n",
    "$$\n",
    "v_0(s_1) = 10.0, \\quad v_0(s_2) = 1.0, \\quad v_0(s_3) = 0.0.\n",
    "$$\n",
    "\n",
    "The value iteration update equation is:\n",
    "\n",
    "$$\n",
    "v_k(s) = \\max_a \\left[ \\mathcal{R}(s, a) + \\gamma \\sum_{s'} \\mathcal{P}(s, a, s') v_{k-1}(s') \\right].\n",
    "$$\n",
    "\n",
    "#### First iteration :\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "v_1(s_1) = \\max(q_1(s_1, a_1), q_1(s_1, a_2)) = \\max(11.15, 11.4) = 11.4.\n",
    "$$\n",
    "$$\n",
    "v_1(s_2) = \\max(q_1(s_2, a_1), q_1(s_2, a_2)) = \\max(4.15, 2.05) = 4.15.\n",
    "$$\n",
    "$$\n",
    "v_1(s_3) = 0.0.\n",
    "$$\n",
    "And the the greedy policy can be compute as $\n",
    "\\pi_1(s_1) = a_2, \\quad \\pi_1(s_2) = a_1, \\quad \\pi_1(s_3) = \\text{N/A (terminal)}.\n",
    "$\n",
    "\n",
    "#### Second iteration :\n",
    "$$\n",
    "v_2(s_1) = \\max(q_2(s_1, a_1), q_2(s_1, a_2)) = \\max(13.5475, 12.8) = 13.5475.\n",
    "$$\n",
    "$$\n",
    "v_2(s_2) = \\max(q_2(s_2, a_1), q_2(s_2, a_2)) = \\max(5.0425, 4.1325) = 5.0425.\n",
    "$$\n",
    "$$\n",
    "v_2(s_3) = 0.0.\n",
    "$$\n",
    "\n",
    "And the the greedy policy can be compute as $\\pi_2(s_1) = a_1, \\quad \\pi_2(s_2) = a_1, \\quad \\pi_2(s_3) = \\text{N/A (terminal)}$.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer:  \n",
    "\n",
    "For the state **$\\mathbf{s_1}$**, the difference between the $q_k$ values for actions $a_1$ and $a_2$ can be expressed as:\n",
    "$$\n",
    "q_k(s_1, a_1) - q_k(s_1, a_2) = \\big(R(s_1, a_1) - R(s_1, a_2)\\big) + \\gamma \\sum_{s'} \\big(P(s_1, a_1, s') - P(s_1, a_2, s')\\big) v_{k-1}(s').\n",
    "$$\n",
    "The immediate reward difference is $R(s_1, a_1) - R(s_1, a_2) = -2$, meaning action $a_2$ starts with a reward advantage of 2 over $a_1$. However, the remaining term, which involves the weighted sum of the values $v_{k-1}(s')$, depends on the transition probabilities and favors $a_1$. Specifically, for the non-zero value states $s_1$ and $s_2$, the probabilities $P(s_1, a_1, s')$ are larger than $P(s_1, a_2, s')$. For $s_3$, the probabilities are reversed, but since $v_{k-1}(s_3) = 0$, this term does not contribute to the difference.\n",
    "\n",
    "At iteration 2, we observe that the weighted sum term exceeds the initial reward advantage of 2 for $a_2$, as shown by the calculations:\n",
    "$$\n",
    "q_2(s_1, a_1) = 13.55, \\quad q_2(s_1, a_2) = 12.8,\n",
    "$$\n",
    "where $q_2(s_1, a_1) > q_2(s_1, a_2)$. This establishes that action $a_1$ becomes preferred by iteration 2. Moreover, since the value function $v_k(s)$ is non-decreasing with each iteration, the weighted sum term will either stay the same or increase, further reinforcing the dominance of $a_1$ over $a_2$ for $k > 2$. Once $q_k(s_1, a_1)$ surpasses $q_k(s_1, a_2)$ by more than 2, the inequality holds for all subsequent iterations because the probabilities associated with $a_1$ remain larger for the non-zero value states.\n",
    "\n",
    "Thus, we conclude that for $k > 2$, action $a_1$ will always be chosen for $s_1$, and the policy $\\pi_k(s_1) = a_1$ remains optimal. This argument shows that $\\pi_2(s_1) = a_1$ is the optimal deterministic policy for $s_1$, and this choice will not change in future iterations.\n",
    "\n",
    "For the state **$\\mathbf{s_2}$** the answer is more straightforward. We observe that the policy $\\pi_k(s_2)$ has remained the same for two consecutive iterations ($\\pi_2(s_2) = \\pi_3(s_2) = \\pi_4(s_2) = a_1$). This indicates that the greedy policy has stabilized, meaning that action $a_1$ consistently provides the highest $q_k(s_2, a)$ value. \n",
    "\n",
    "Additionally, the computed $q_k(s_2, a_1)$ and $q_k(s_2, a_2)$ values are becoming closer with each iteration. This decreasing difference between $q_k(s_2, a_1)$ and $q_k(s_2, a_2)$ reflects the fact that the value function $v_k(s_2)$ is converging, and the influence of future updates on the policy decision is diminishing. Since the greedy policy $\\pi_k(s_2)$ selects the action that maximizes $q_k(s_2, a)$, and this choice has stabilized across iterations, we can conclude that the policy $\\pi_k(s_2) = a_1$ is fixed for all $k > 2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer:  \n",
    "\n",
    "We know that we have $\\pi^*(s) = a_1$ so we must have $v^* = R(a_1)+P(a_1)v^*$, which leads to $v^* = (I_2-P(a_1))^{-1}R(a_1)$, and finally we have $v^*(s_1) = 16.8361$ and $v^*(s_2) = 7.11865$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (D) Answer\n",
    "\n",
    "#### Value Iteration:  \n",
    "\n",
    "We initialize the value function as:\n",
    "\n",
    "$$\n",
    "v_0(s_1) = 10.0, \\quad v_0(s_2) = 1.0, \\quad v_0(s_3) = 0.0.\n",
    "$$\n",
    "\n",
    "The value iteration update equation is:\n",
    "\n",
    "$$\n",
    "v_k(s) = \\max_a \\left[ \\mathcal{R}(s, a) + \\gamma \\sum_{s'} \\mathcal{P}(s, a, s') v_{k-1}(s') \\right].\n",
    "$$\n",
    "\n",
    "#### First iteration :\n",
    "\n",
    "\n",
    "$$\n",
    "v_1(s_1) = \\max(q_1(s_1, a_1), q_1(s_1, a_2)) = \\max(11.15, 12.4) = 12.4.\n",
    "$$\n",
    "$$\n",
    "v_1(s_2) = \\max(q_1(s_2, a_1), q_1(s_2, a_2)) = \\max(4.15, 2.05) = 4.15.\n",
    "$$\n",
    "$$\n",
    "v_1(s_3) = 0.0.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Optimal Deterministic Policy:  \n",
    "\n",
    "We see that the greedy policy at iteration $1$ is $\\pi_1(s_1) = a_2$ and $\\pi_1(s_2) = a_1$, hence it will change the final policy because this time $\\pi_k(s_1)$ remains $a_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Fixed-Point and Policy Evaluation True/False Questions (Led by Corentin)\n",
    "\n",
    "### Recall Section: Key Formulas and Definitions\n",
    "\n",
    "#### Bellman Optimality Equation\n",
    "The Bellman Optimality Equation for state-value functions is:\n",
    "$$\n",
    "V^*(s) = \\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s, a, s') V^*(s') \\right].\n",
    "$$\n",
    "For action-value functions:\n",
    "$$\n",
    "Q^*(s, a) = R(s, a) + \\gamma \\sum_{s'} P(s, a, s') \\max_{a'} Q^*(s', a').\n",
    "$$\n",
    "\n",
    "#### Contraction Property\n",
    "The Bellman Policy Operator $B^\\pi$ is a contraction under the $L^\\infty$-norm:\n",
    "$$\n",
    "\\|B^\\pi(X) - B^\\pi(Y)\\|_\\infty \\leq \\gamma \\|X - Y\\|_\\infty.\n",
    "$$\n",
    "This guarantees convergence to a unique fixed point.\n",
    "\n",
    "#### Policy Iteration\n",
    "Policy Iteration alternates between:\n",
    "1. **Policy Evaluation**: Compute $V^\\pi$ for the current policy $\\pi$.\n",
    "2. **Policy Improvement**: Generate a new policy $\\pi'$ by setting:\n",
    "   $$\n",
    "   \\pi'(s) = \\arg\\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s, a, s') V^\\pi(s') \\right].\n",
    "   $$\n",
    "\n",
    "#### Discounted Return\n",
    "The discounted return from time step $t$ is:\n",
    "$$\n",
    "G_t = \\sum_{i=t+1}^\\infty \\gamma^{i-t-1} R_i,\n",
    "$$\n",
    "where $\\gamma \\in [0, 1)$ is the discount factor.\n",
    "\n",
    "### True/False Questions (Provide Justification)\n",
    "\n",
    "1. **True/False**: If $Q^\\pi(s, a) = 5$, $P(s, a, s') = 0.5$ for $s' \\in \\{s_1, s_2\\}$, and the immediate reward $R(s, a)$ increases by $2$, the updated action-value function $Q^\\pi(s, a)$ also increases by $2$.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "2. **True/False**: For a discount factor $\\gamma = 0.9$, the discounted return for rewards $R_1 = 5, R_2 = 3, R_3 = 1$ is greater than $6$.\n",
    "\n",
    "---\n",
    "\n",
    "3. **True/False**: The Bellman Policy Operator $B^\\pi(V) = R^\\pi + \\gamma P^\\pi \\cdot V$ satisfies the contraction property for all $\\gamma \\in [0, 1)$, ensuring a unique fixed point.\n",
    "\n",
    "---\n",
    "\n",
    "4. **True/False**: In Policy Iteration, the Policy Improvement step guarantees that the updated policy $\\pi'$ will always perform strictly better than the previous policy $\\pi$.\n",
    "\n",
    "---\n",
    "\n",
    "5. **True/False**: If $Q^\\pi(s, a) = 10$ for all actions $a$ in a state $s$, then the corresponding state-value function $V^\\pi(s) = 10$, regardless of the policy $\\pi$.\n",
    "\n",
    "---\n",
    "\n",
    "6. **True/False**: The discounted return $G_t = \\sum_{i=t+1}^\\infty \\gamma^{i-t-1} R_i$ converges to a finite value for any sequence of bounded rewards if $\\gamma < 1$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers (Provide justification, brief explanations are fine)\n",
    "\n",
    "#### Question 1:  \n",
    "\n",
    "True, we know that $Q^\\pi(s,a) = R(s,a)+\\gamma\\sum_{s'}P(s,a,s')V^\\pi(s')$, hence if $R(s,a)$ is increased by $2$ then $Q^\\pi(s,a)$ is also increased by $2$.\n",
    "\n",
    "#### Question 2:  \n",
    "\n",
    "True, we have $G_t = 5+0.9*3+0.9^2*1 = 8.51\\geq 6$\n",
    "\n",
    "#### Question 3:  \n",
    "\n",
    "True, we have $||B^pi(X)-B^\\pi(Y)||_\\infty\\leq \\gamma||X-Y||_\\infty$\n",
    "\n",
    "#### Question 4:  \n",
    "\n",
    "False, in Policy Iteration, the Policy Improvement step guarantees that the updated policy $\\pi'$ is at least as good as the previous policy $\\pi$, but it does not always perform strictly better. If the current policy is already optimal, the policy improvement step will not change the policy.\n",
    "\n",
    "\n",
    "#### Question 5:  \n",
    "\n",
    "True, if $Q^\\pi(s,a) = 10$ for all actions $a$, then $V^\\pi(s) = \\sum\\pi(s,a)Q^\\pi(s,a) = 10\\sum\\pi(s,a)=10$\n",
    "\n",
    "#### Question 6:  \n",
    "\n",
    "True, if $\\gamma <1$ and $R_t$ are bounded then $G_t$ converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
