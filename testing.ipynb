{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Process1:\n",
    "    @dataclass\n",
    "    class State:\n",
    "        price: int\n",
    "\n",
    "    level_param: int  # level to which price mean-reverts\n",
    "    alpha1: float = 0.25  # strength of mean-reversion (non-negative value)\n",
    "\n",
    "    def up_prob(self, state: State) -> float:\n",
    "        return 1. / (1+np.exp(-self.alpha1*(state.price - self.level_param)))\n",
    "\n",
    "    def next_state(self, state: State) -> State:\n",
    "        up_move: int = np.random.binomial(1, self.up_prob(state), 1)[0]\n",
    "        return Process1.State(price=state.price + up_move * 2 - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation(process, start_state):\n",
    "  state = start_state \n",
    "  while True: \n",
    "    yield state \n",
    "    state = process.next_state(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools \n",
    "\n",
    "def process1_price_traces(\n",
    "  start_price: int, \n",
    "  level_param: int, \n",
    "  alpha1: float, \n",
    "  time_steps: int, \n",
    "  num_traces: int \n",
    ") -> np.ndarray: \n",
    "  process = Process1(level_param=level_param, alpha1=alpha1)\n",
    "  start_state = Process1.State(price=start_price)\n",
    "  return np.vstack([\n",
    "    np.fromiter((s.price for s in itertools.islice(\n",
    "      simulation(process, start_state), \n",
    "      time_steps + 1\n",
    "    )), float) for _ in range(num_traces)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_price: int = 100\n",
    "level_param: int = 100\n",
    "alpha1: float = 0.25\n",
    "alpha2: float = 0.75\n",
    "alpha3: float = 1.0\n",
    "time_steps: int = 100\n",
    "num_traces: int = 1000\n",
    "\n",
    "process1_traces: np.ndarray = process1_price_traces(\n",
    "    start_price=start_price,\n",
    "    level_param=level_param,\n",
    "    alpha1=alpha1,\n",
    "    time_steps=time_steps,\n",
    "    num_traces=num_traces\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100. 101. 100. ... 186. 187. 188.]\n",
      " [100. 101. 102. ...  32.  31.  30.]\n",
      " [100.  99. 100. ...   8.   7.   6.]\n",
      " ...\n",
      " [100.  99.  98. ...   6.   5.   4.]\n",
      " [100.  99.  98. ... 192. 193. 194.]\n",
      " [100.  99.  98. ...   6.   5.   4.]]\n"
     ]
    }
   ],
   "source": [
    "print(process1_traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP Transition Map\n",
      "------------------\n",
      "From State InventoryState(on_hand=0, on_order=0):\n",
      "  With Action 0:\n",
      "    To [State InventoryState(on_hand=0, on_order=0) and Reward -10.000] with Probability 1.000\n",
      "  With Action 1:\n",
      "    To [State InventoryState(on_hand=0, on_order=1) and Reward -10.000] with Probability 1.000\n",
      "  With Action 2:\n",
      "    To [State InventoryState(on_hand=0, on_order=2) and Reward -10.000] with Probability 1.000\n",
      "From State InventoryState(on_hand=0, on_order=1):\n",
      "  With Action 0:\n",
      "    To [State InventoryState(on_hand=1, on_order=0) and Reward -0.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=0, on_order=0) and Reward -5.820] with Probability 0.632\n",
      "  With Action 1:\n",
      "    To [State InventoryState(on_hand=1, on_order=1) and Reward -0.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=0, on_order=1) and Reward -5.820] with Probability 0.632\n",
      "From State InventoryState(on_hand=0, on_order=2):\n",
      "  With Action 0:\n",
      "    To [State InventoryState(on_hand=2, on_order=0) and Reward -0.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=1, on_order=0) and Reward -0.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=0, on_order=0) and Reward -3.922] with Probability 0.264\n",
      "From State InventoryState(on_hand=1, on_order=0):\n",
      "  With Action 0:\n",
      "    To [State InventoryState(on_hand=1, on_order=0) and Reward -1.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=0, on_order=0) and Reward -6.820] with Probability 0.632\n",
      "  With Action 1:\n",
      "    To [State InventoryState(on_hand=1, on_order=1) and Reward -1.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=0, on_order=1) and Reward -6.820] with Probability 0.632\n",
      "From State InventoryState(on_hand=1, on_order=1):\n",
      "  With Action 0:\n",
      "    To [State InventoryState(on_hand=2, on_order=0) and Reward -1.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=1, on_order=0) and Reward -1.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=0, on_order=0) and Reward -4.922] with Probability 0.264\n",
      "From State InventoryState(on_hand=2, on_order=0):\n",
      "  With Action 0:\n",
      "    To [State InventoryState(on_hand=2, on_order=0) and Reward -2.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=1, on_order=0) and Reward -2.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=0, on_order=0) and Reward -5.922] with Probability 0.264\n",
      "\n",
      "Deterministic Policy Map\n",
      "------------------------\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 2\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n",
      "Implied MP Transition Map\n",
      "--------------\n",
      "From State InventoryState(on_hand=0, on_order=0):\n",
      "  To State InventoryState(on_hand=0, on_order=2) with Probability 1.000\n",
      "From State InventoryState(on_hand=0, on_order=1):\n",
      "  To State InventoryState(on_hand=1, on_order=1) with Probability 0.368\n",
      "  To State InventoryState(on_hand=0, on_order=1) with Probability 0.632\n",
      "From State InventoryState(on_hand=0, on_order=2):\n",
      "  To State InventoryState(on_hand=2, on_order=0) with Probability 0.368\n",
      "  To State InventoryState(on_hand=1, on_order=0) with Probability 0.368\n",
      "  To State InventoryState(on_hand=0, on_order=0) with Probability 0.264\n",
      "From State InventoryState(on_hand=1, on_order=0):\n",
      "  To State InventoryState(on_hand=1, on_order=1) with Probability 0.368\n",
      "  To State InventoryState(on_hand=0, on_order=1) with Probability 0.632\n",
      "From State InventoryState(on_hand=1, on_order=1):\n",
      "  To State InventoryState(on_hand=2, on_order=0) with Probability 0.368\n",
      "  To State InventoryState(on_hand=1, on_order=0) with Probability 0.368\n",
      "  To State InventoryState(on_hand=0, on_order=0) with Probability 0.264\n",
      "From State InventoryState(on_hand=2, on_order=0):\n",
      "  To State InventoryState(on_hand=2, on_order=0) with Probability 0.368\n",
      "  To State InventoryState(on_hand=1, on_order=0) with Probability 0.368\n",
      "  To State InventoryState(on_hand=0, on_order=0) with Probability 0.264\n",
      "\n",
      "Implied MRP Transition Reward Map\n",
      "---------------------\n",
      "From State InventoryState(on_hand=0, on_order=0):\n",
      "  To [State InventoryState(on_hand=0, on_order=2) and Reward -10.000] with Probability 1.000\n",
      "From State InventoryState(on_hand=0, on_order=1):\n",
      "  To [State InventoryState(on_hand=1, on_order=1) and Reward -0.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=0, on_order=1) and Reward -5.820] with Probability 0.632\n",
      "From State InventoryState(on_hand=0, on_order=2):\n",
      "  To [State InventoryState(on_hand=2, on_order=0) and Reward -0.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=1, on_order=0) and Reward -0.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=0, on_order=0) and Reward -3.922] with Probability 0.264\n",
      "From State InventoryState(on_hand=1, on_order=0):\n",
      "  To [State InventoryState(on_hand=1, on_order=1) and Reward -1.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=0, on_order=1) and Reward -6.820] with Probability 0.632\n",
      "From State InventoryState(on_hand=1, on_order=1):\n",
      "  To [State InventoryState(on_hand=2, on_order=0) and Reward -1.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=1, on_order=0) and Reward -1.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=0, on_order=0) and Reward -4.922] with Probability 0.264\n",
      "From State InventoryState(on_hand=2, on_order=0):\n",
      "  To [State InventoryState(on_hand=2, on_order=0) and Reward -2.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=1, on_order=0) and Reward -2.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=0, on_order=0) and Reward -5.922] with Probability 0.264\n",
      "\n",
      "Implied MP Stationary Distribution\n",
      "-----------------------\n",
      "{InventoryState(on_hand=1, on_order=1): np.float64(0.162),\n",
      " InventoryState(on_hand=0, on_order=0): np.float64(0.117),\n",
      " InventoryState(on_hand=0, on_order=1): np.float64(0.279),\n",
      " InventoryState(on_hand=2, on_order=0): np.float64(0.162),\n",
      " InventoryState(on_hand=1, on_order=0): np.float64(0.162),\n",
      " InventoryState(on_hand=0, on_order=2): np.float64(0.117)}\n",
      "\n",
      "Implied MRP Reward Function\n",
      "---------------\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): np.float64(-10.0),\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): np.float64(-3.036),\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): np.float64(-1.036),\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): np.float64(-4.679),\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): np.float64(-3.679),\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): np.float64(-2.036)}\n",
      "\n",
      "Implied MRP Value Function\n",
      "--------------\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): np.float64(-43.596),\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): np.float64(-39.329),\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): np.float64(-37.329),\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): np.float64(-38.971),\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): np.float64(-37.971),\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): np.float64(-38.329)}\n",
      "\n",
      "Implied MRP Policy Evaluation Value Function\n",
      "--------------\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): np.float64(-43.59563313047815),\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): np.float64(-39.3284904356655),\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): np.float64(-37.3284904356655),\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): np.float64(-38.97111179441265),\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): np.float64(-37.97111179441265),\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): np.float64(-38.3284904356655)}\n",
      "\n",
      "MDP Policy Iteration Optimal Value Function and Optimal Policy\n",
      "--------------\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): np.float64(-43.59563313047815),\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): np.float64(-37.97111179441265),\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): np.float64(-37.3284904356655),\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): np.float64(-38.97111179441265),\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): np.float64(-38.3284904356655),\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): np.float64(-39.3284904356655)}\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 2\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n",
      "\n",
      "MDP Value Iteration Optimal Value Function and Optimal Policy\n",
      "--------------\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): np.float64(-43.59563313047815),\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): np.float64(-37.97111179441265),\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): np.float64(-39.3284904356655),\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): np.float64(-38.97111179441265),\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): np.float64(-37.3284904356655),\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): np.float64(-38.3284904356655)}\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 2\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict, Mapping\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "from rl.policy import FiniteDeterministicPolicy\n",
    "from rl.markov_process import FiniteMarkovProcess, FiniteMarkovRewardProcess\n",
    "from rl.distribution import Categorical\n",
    "from scipy.stats import poisson\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class InventoryState:\n",
    "    on_hand: int\n",
    "    on_order: int\n",
    "\n",
    "    def inventory_position(self) -> int:\n",
    "        return self.on_hand + self.on_order\n",
    "\n",
    "\n",
    "InvOrderMapping = Mapping[\n",
    "    InventoryState,\n",
    "    Mapping[int, Categorical[Tuple[InventoryState, float]]]\n",
    "]\n",
    "\n",
    "\n",
    "class SimpleInventoryMDPCap(FiniteMarkovDecisionProcess[InventoryState, int]):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        capacity: int,\n",
    "        poisson_lambda: float,\n",
    "        holding_cost: float,\n",
    "        stockout_cost: float\n",
    "    ):\n",
    "        self.capacity: int = capacity\n",
    "        self.poisson_lambda: float = poisson_lambda\n",
    "        self.holding_cost: float = holding_cost\n",
    "        self.stockout_cost: float = stockout_cost\n",
    "\n",
    "        self.poisson_distr = poisson(poisson_lambda)\n",
    "        super().__init__(self.get_action_transition_reward_map())\n",
    "\n",
    "    def get_action_transition_reward_map(self) -> InvOrderMapping:\n",
    "        d: Dict[InventoryState, Dict[int, Categorical[Tuple[InventoryState,\n",
    "                                                            float]]]] = {}\n",
    "\n",
    "        for alpha in range(self.capacity + 1):\n",
    "            for beta in range(self.capacity + 1 - alpha):\n",
    "                state: InventoryState = InventoryState(alpha, beta)\n",
    "                ip: int = state.inventory_position()\n",
    "                base_reward: float = - self.holding_cost * alpha\n",
    "                d1: Dict[int, Categorical[Tuple[InventoryState, float]]] = {}\n",
    "\n",
    "                for order in range(self.capacity - ip + 1):\n",
    "                    sr_probs_dict: Dict[Tuple[InventoryState, float], float] =\\\n",
    "                        {(InventoryState(ip - i, order), base_reward):\n",
    "                        self.poisson_distr.pmf(i) for i in range(ip)}\n",
    "\n",
    "                    probability: float = 1 - self.poisson_distr.cdf(ip - 1)\n",
    "                    reward: float = base_reward - self.stockout_cost * \\\n",
    "                        (self.poisson_lambda - ip * \n",
    "                        (1 - self.poisson_distr.pmf(ip) / probability))\n",
    "                    sr_probs_dict[(InventoryState(0, order), reward)] = \\\n",
    "                        probability\n",
    "                    d1[order] = Categorical(sr_probs_dict)\n",
    "\n",
    "                d[state] = d1\n",
    "        return d\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from pprint import pprint\n",
    "\n",
    "    user_capacity = 2\n",
    "    user_poisson_lambda = 1.0\n",
    "    user_holding_cost = 1.0\n",
    "    user_stockout_cost = 10.0\n",
    "\n",
    "    user_gamma = 0.9\n",
    "\n",
    "    si_mdp: FiniteMarkovDecisionProcess[InventoryState, int] =\\\n",
    "        SimpleInventoryMDPCap(\n",
    "            capacity=user_capacity,\n",
    "            poisson_lambda=user_poisson_lambda,\n",
    "            holding_cost=user_holding_cost,\n",
    "            stockout_cost=user_stockout_cost\n",
    "        )\n",
    "\n",
    "    print(\"MDP Transition Map\")\n",
    "    print(\"------------------\")\n",
    "    print(si_mdp)\n",
    "\n",
    "    fdp: FiniteDeterministicPolicy[InventoryState, int] = \\\n",
    "        FiniteDeterministicPolicy(\n",
    "            {InventoryState(alpha, beta): user_capacity - (alpha + beta)\n",
    "            for alpha in range(user_capacity + 1)\n",
    "            for beta in range(user_capacity + 1 - alpha)}\n",
    "    )\n",
    "\n",
    "    print(\"Deterministic Policy Map\")\n",
    "    print(\"------------------------\")\n",
    "    print(fdp)\n",
    "\n",
    "    implied_mrp: FiniteMarkovRewardProcess[InventoryState] =\\\n",
    "        si_mdp.apply_finite_policy(fdp)\n",
    "    print(\"Implied MP Transition Map\")\n",
    "    print(\"--------------\")\n",
    "    print(FiniteMarkovProcess(\n",
    "        {s.state: Categorical({s1.state: p for s1, p in v.table().items()})\n",
    "        for s, v in implied_mrp.transition_map.items()}\n",
    "    ))\n",
    "\n",
    "    print(\"Implied MRP Transition Reward Map\")\n",
    "    print(\"---------------------\")\n",
    "    print(implied_mrp)\n",
    "\n",
    "    print(\"Implied MP Stationary Distribution\")\n",
    "    print(\"-----------------------\")\n",
    "    implied_mrp.display_stationary_distribution()\n",
    "    print()\n",
    "\n",
    "    print(\"Implied MRP Reward Function\")\n",
    "    print(\"---------------\")\n",
    "    implied_mrp.display_reward_function()\n",
    "    print()\n",
    "\n",
    "    print(\"Implied MRP Value Function\")\n",
    "    print(\"--------------\")\n",
    "    implied_mrp.display_value_function(gamma=user_gamma)\n",
    "    print()\n",
    "\n",
    "    from rl.dynamic_programming import evaluate_mrp_result\n",
    "    from rl.dynamic_programming import policy_iteration_result\n",
    "    from rl.dynamic_programming import value_iteration_result\n",
    "\n",
    "    print(\"Implied MRP Policy Evaluation Value Function\")\n",
    "    print(\"--------------\")\n",
    "    pprint(evaluate_mrp_result(implied_mrp, gamma=user_gamma))\n",
    "    print()\n",
    "\n",
    "    print(\"MDP Policy Iteration Optimal Value Function and Optimal Policy\")\n",
    "    print(\"--------------\")\n",
    "    opt_vf_pi, opt_policy_pi = policy_iteration_result(\n",
    "        si_mdp,\n",
    "        gamma=user_gamma\n",
    "    )\n",
    "    pprint(opt_vf_pi)\n",
    "    print(opt_policy_pi)\n",
    "    print()\n",
    "\n",
    "    print(\"MDP Value Iteration Optimal Value Function and Optimal Policy\")\n",
    "    print(\"--------------\")\n",
    "    opt_vf_vi, opt_policy_vi = value_iteration_result(si_mdp, gamma=user_gamma)\n",
    "    pprint(opt_vf_vi)\n",
    "    print(opt_policy_vi)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is one acceptable answer. One possible solution was to “lift‐and‐shift” the single–store code into a new MDP whose state now is a pair of inventory states (one for each store) and whose actions are a triple giving (order₁, order₂, transfer) where the “order” components are subject to the usual capacity constraints and the “transfer” decision (positive meaning “from store 1 to store 2” and negative meaning “from store 2 to store 1”) is chosen from the physically feasible set (you cannot send more than a store’s current on–hand inventory). In addition, fixed ordering cost (K₁) is incurred at a store if its order is nonzero and a fixed transfer cost (K₂) is incurred whenever a nonzero transfer is made. (The cost‐penalties for holding and for stockouts remain “local” to each store.) One acceptable solution is shown below.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "TwoStoresInventoryMDP.py\n",
    "\n",
    "We model a two–store inventory control problem as a Finite MDP. Each store has its\n",
    "own capacity, Poisson demand (with its own mean), holding cost and stockout cost.\n",
    "At “6pm” each store may order inventory (subject to the constraint that the store’s\n",
    "inventory position, defined as on_hand + on_order, does not exceed capacity) and\n",
    "an inventory transfer between the stores is also possible (with a fixed cost K₂ if\n",
    "a nonzero transfer occurs). Orders incur a fixed cost K₁. (All costs are taken as penalties.)\n",
    "We then solve for the optimal value function and policy using standard DP methods.\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict, Mapping\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "from rl.distribution import Categorical\n",
    "from scipy.stats import poisson\n",
    "\n",
    "# We re–use the same InventoryState as in the single–store example.\n",
    "@dataclass(frozen=True)\n",
    "class InventoryState:\n",
    "    on_hand: int\n",
    "    on_order: int\n",
    "\n",
    "    def inventory_position(self) -> int:\n",
    "        return self.on_hand + self.on_order\n",
    "\n",
    "# The state of the two–store system is just a pair of InventoryStates.\n",
    "@dataclass(frozen=True)\n",
    "class TwoStoreInventoryState:\n",
    "    store1: InventoryState\n",
    "    store2: InventoryState\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"(Store1: {self.store1}, Store2: {self.store2})\"\n",
    "\n",
    "# For clarity we define our action type to be a triple:\n",
    "# (order1, order2, transfer)\n",
    "# where transfer > 0 means transferring that many units from store 1 to store 2,\n",
    "# transfer < 0 means transferring |transfer| units from store 2 to store 1.\n",
    "TwoStoreAction = Tuple[int, int, int]\n",
    "\n",
    "# Type alias for the mapping (from states to action maps).\n",
    "TwoStoreMapping = Mapping[\n",
    "    TwoStoreInventoryState,\n",
    "    Mapping[TwoStoreAction, Categorical[Tuple[TwoStoreInventoryState, float]]]\n",
    "]\n",
    "\n",
    "class TwoStoresInventoryMDP(FiniteMarkovDecisionProcess[TwoStoreInventoryState, TwoStoreAction]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        capacity1: int,\n",
    "        capacity2: int,\n",
    "        poisson_lambda1: float,\n",
    "        poisson_lambda2: float,\n",
    "        holding_cost1: float,\n",
    "        holding_cost2: float,\n",
    "        stockout_cost1: float,\n",
    "        stockout_cost2: float,\n",
    "        fixed_order_cost: float,\n",
    "        fixed_transfer_cost: float\n",
    "    ):\n",
    "        self.capacity1 = capacity1\n",
    "        self.capacity2 = capacity2\n",
    "        self.poisson_lambda1 = poisson_lambda1\n",
    "        self.poisson_lambda2 = poisson_lambda2\n",
    "        self.holding_cost1 = holding_cost1\n",
    "        self.holding_cost2 = holding_cost2\n",
    "        self.stockout_cost1 = stockout_cost1\n",
    "        self.stockout_cost2 = stockout_cost2\n",
    "        self.fixed_order_cost = fixed_order_cost\n",
    "        self.fixed_transfer_cost = fixed_transfer_cost\n",
    "\n",
    "        # Pre‐compute the Poisson distributions.\n",
    "        self.poisson1 = poisson(poisson_lambda1)\n",
    "        self.poisson2 = poisson(poisson_lambda2)\n",
    "        super().__init__(self.get_action_transition_reward_map())\n",
    "\n",
    "    def get_action_transition_reward_map(self) -> TwoStoreMapping:\n",
    "        mapping: Dict[TwoStoreInventoryState,\n",
    "                      Dict[TwoStoreAction, Categorical[Tuple[TwoStoreInventoryState, float]]]] = {}\n",
    "\n",
    "        # Enumerate over all possible states for store 1 and store 2.\n",
    "        for on_hand1 in range(self.capacity1 + 1):\n",
    "            for on_order1 in range(self.capacity1 - on_hand1 + 1):\n",
    "                state1 = InventoryState(on_hand1, on_order1)\n",
    "                for on_hand2 in range(self.capacity2 + 1):\n",
    "                    for on_order2 in range(self.capacity2 - on_hand2 + 1):\n",
    "                        state2 = InventoryState(on_hand2, on_order2)\n",
    "                        state = TwoStoreInventoryState(state1, state2)\n",
    "\n",
    "                        # Determine feasible orders.\n",
    "                        max_order1 = self.capacity1 - state1.inventory_position()\n",
    "                        max_order2 = self.capacity2 - state2.inventory_position()\n",
    "                        orders1 = list(range(max_order1 + 1))\n",
    "                        orders2 = list(range(max_order2 + 1))\n",
    "\n",
    "                        # Determine feasible transfers.\n",
    "                        # We allow transfer from store 1 to store 2 up to state1.on_hand,\n",
    "                        # or from store 2 to store 1 up to state2.on_hand.\n",
    "                        transfer_set = {0}\n",
    "                        for t in range(1, state1.on_hand + 1):\n",
    "                            transfer_set.add(t)\n",
    "                        for t in range(1, state2.on_hand + 1):\n",
    "                            transfer_set.add(-t)\n",
    "\n",
    "                        action_dict: Dict[TwoStoreAction, Categorical[Tuple[TwoStoreInventoryState, float]]] = {}\n",
    "                        for order1 in orders1:\n",
    "                            for order2 in orders2:\n",
    "                                for transfer in transfer_set:\n",
    "                                    action: TwoStoreAction = (order1, order2, transfer)\n",
    "                                    # Compute fixed costs.\n",
    "                                    cost = 0.0\n",
    "                                    if order1 > 0:\n",
    "                                        cost += self.fixed_order_cost\n",
    "                                    if order2 > 0:\n",
    "                                        cost += self.fixed_order_cost\n",
    "                                    if transfer != 0:\n",
    "                                        cost += self.fixed_transfer_cost\n",
    "                                    # Also, incur holding costs on the current on‐hand levels.\n",
    "                                    cost += self.holding_cost1 * state1.on_hand\n",
    "                                    cost += self.holding_cost2 * state2.on_hand\n",
    "                                    base_reward = -cost\n",
    "\n",
    "                                    # The available inventory to meet demand is the store’s inventory position\n",
    "                                    # adjusted by any transfer.\n",
    "                                    ip1 = state1.inventory_position()\n",
    "                                    ip2 = state2.inventory_position()\n",
    "                                    if transfer >= 0:\n",
    "                                        # transferring from store1 to store2\n",
    "                                        ip1_prime = ip1 - transfer\n",
    "                                        ip2_prime = ip2 + transfer\n",
    "                                    else:\n",
    "                                        t_abs = -transfer\n",
    "                                        ip1_prime = ip1 + t_abs\n",
    "                                        ip2_prime = ip2 - t_abs\n",
    "\n",
    "                                    # For each store, we mimic the single–store dynamics:\n",
    "                                    # (i) For demand d < available inventory, new on–hand is (available – d);\n",
    "                                    # (ii) For demand d >= available inventory, we lump the tail with an extra penalty.\n",
    "                                    outcomes1: Dict[Tuple[int, float], float] = {}\n",
    "                                    for d1 in range(ip1_prime):\n",
    "                                        new_on_hand1 = ip1_prime - d1\n",
    "                                        outcomes1[(new_on_hand1, 0.0)] = self.poisson1.pmf(d1)\n",
    "                                    tail_prob1 = 1 - self.poisson1.cdf(ip1_prime - 1) if ip1_prime > 0 else 1.0\n",
    "                                    if tail_prob1 > 0:\n",
    "                                        # Following the same “aggregation” used in the single–store code.\n",
    "                                        adj1 = - self.stockout_cost1 * (\n",
    "                                            self.poisson_lambda1 - ip1_prime * (1 - self.poisson1.pmf(ip1_prime) / tail_prob1)\n",
    "                                        )\n",
    "                                        outcomes1[(0, adj1)] = tail_prob1\n",
    "\n",
    "                                    outcomes2: Dict[Tuple[int, float], float] = {}\n",
    "                                    for d2 in range(ip2_prime):\n",
    "                                        new_on_hand2 = ip2_prime - d2\n",
    "                                        outcomes2[(new_on_hand2, 0.0)] = self.poisson2.pmf(d2)\n",
    "                                    tail_prob2 = 1 - self.poisson2.cdf(ip2_prime - 1) if ip2_prime > 0 else 1.0\n",
    "                                    if tail_prob2 > 0:\n",
    "                                        adj2 = - self.stockout_cost2 * (\n",
    "                                            self.poisson_lambda2 - ip2_prime * (1 - self.poisson2.pmf(ip2_prime) / tail_prob2)\n",
    "                                        )\n",
    "                                        outcomes2[(0, adj2)] = tail_prob2\n",
    "\n",
    "                                    # Combine outcomes for store1 and store2 (assuming independent demands).\n",
    "                                    sr_probs: Dict[Tuple[TwoStoreInventoryState, float], float] = {}\n",
    "                                    for (new_on_hand1, adj1), prob1 in outcomes1.items():\n",
    "                                        for (new_on_hand2, adj2), prob2 in outcomes2.items():\n",
    "                                            next_state = TwoStoreInventoryState(\n",
    "                                                InventoryState(new_on_hand1, order1),\n",
    "                                                InventoryState(new_on_hand2, order2)\n",
    "                                            )\n",
    "                                            total_reward = base_reward + adj1 + adj2\n",
    "                                            key = (next_state, total_reward)\n",
    "                                            sr_probs[key] = sr_probs.get(key, 0.0) + prob1 * prob2\n",
    "\n",
    "                                    action_dict[action] = Categorical(sr_probs)\n",
    "                        mapping[state] = action_dict\n",
    "        return mapping\n",
    "\n",
    "# A simple test and policy computation using value iteration.\n",
    "if __name__ == '__main__':\n",
    "    from rl.dynamic_programming import value_iteration_result\n",
    "\n",
    "    # Discount factor.\n",
    "    gamma = 0.9\n",
    "\n",
    "    # Example problem parameters.\n",
    "    capacity1 = 3\n",
    "    capacity2 = 3\n",
    "    poisson_lambda1 = 1.0\n",
    "    poisson_lambda2 = 2.0\n",
    "    holding_cost1 = 1.0\n",
    "    holding_cost2 = 1.0\n",
    "    stockout_cost1 = 10.0\n",
    "    stockout_cost2 = 10.0\n",
    "    fixed_order_cost = 5.0\n",
    "    fixed_transfer_cost = 2.0\n",
    "\n",
    "    mdp = TwoStoresInventoryMDP(\n",
    "        capacity1, capacity2,\n",
    "        poisson_lambda1, poisson_lambda2,\n",
    "        holding_cost1, holding_cost2,\n",
    "        stockout_cost1, stockout_cost2,\n",
    "        fixed_order_cost, fixed_transfer_cost\n",
    "    )\n",
    "\n",
    "    opt_vf, opt_policy = value_iteration_result(mdp, gamma=gamma)\n",
    "    print(\"Optimal Value Function:\")\n",
    "    for state, value in opt_vf.items():\n",
    "        print(f\"{state}: {value}\")\n",
    "\n",
    "    print(\"\\nOptimal Policy:\")\n",
    "    print(opt_policy)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation and Analysis\n",
    "\n",
    "1. **State and Action spaces:**  \n",
    "   The new state is a pair  \n",
    "   $$\n",
    "   s = \\bigl(\\texttt{InventoryState(on\\_hand₁, on\\_order₁)},\\,\\texttt{InventoryState(on\\_hand₂, on\\_order₂)}\\bigr),\n",
    "   $$\n",
    "   where (on_hand, on_order) for each store satisfy the capacity constraint  \n",
    "   $$\n",
    "   \\text{on\\_hand} + \\text{on\\_order} \\le \\text{Capacity}.\n",
    "   $$\n",
    "   The actions are triples $(\\text{order₁}, \\text{order₂}, \\text{transfer})$ where the ordering decisions obey  \n",
    "   $$\n",
    "   \\text{order}_i \\in \\{0,1,\\dots, \\text{Capacity}_i - (\\text{on\\_hand}_i+\\text{on\\_order}_i)\\},\n",
    "   $$\n",
    "   and the transfer decision is chosen from the feasible set: a store can send at most its available on–hand inventory (so, if store 1 has on_hand = a₁ then it may send any t in 1,…,a₁; similarly, store 2 may send up to its on–hand inventory in the negative direction).\n",
    "\n",
    "2. **Transitions and Rewards:**  \n",
    "   Once an action is chosen the “available inventory” for demand is computed as the store’s current inventory position (on_hand + on_order) adjusted by the transfer. Then, independent Poisson demands are realized at each store. (For computational efficiency the tail of the demand distribution is lumped as in the single–store example.) In addition to the usual “stockout” penalties and holding costs, fixed ordering and transfer costs are subtracted immediately when the corresponding (nonzero) action is taken.\n",
    "\n",
    "3. **Optimal Policy:**  \n",
    "   When the problem is solved (for example by calling the provided `value_iteration_result`), one typically finds that:  \n",
    "   - **Ordering:** A store with low current inventory (or low inventory position) tends to order more in order to “top–up” its inventory, but the fixed ordering cost makes it undesirable to order a little at a time.  \n",
    "   - **Transfers:** When one store has a surplus relative to its demand distribution while the other is relatively “short,” the optimal policy may call for transferring inventory rather than placing an order. (In our example the transfer cost is lower than the ordering cost, so a moderate transfer is often optimal.)  \n",
    "   - **Parameter Sensitivity:** As the fixed order cost increases, the policy tends to delay ordering or rely more on transfers if possible. Conversely, if the demand mean is high (or if stockout penalties are steep), the policy is more aggressive in ordering (and perhaps transferring) to avoid stockouts.\n",
    "\n",
    "This behavior is entirely intuitive. In many real–world settings, managers must balance the trade–offs between incurring fixed order (or shipping) costs versus the risk and cost of stockouts, and sharing inventory between stores is a natural way to do so. The above implementation and its computed optimal policy illustrate these trade–offs in a formal dynamic–programming model.\n",
    "\n",
    "---\n",
    "\n",
    "Any solution that produces a similar MDP model and obtains an optimal policy with these properties is acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
